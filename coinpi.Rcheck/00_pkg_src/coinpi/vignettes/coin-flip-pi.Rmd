---
title: "Computing Pi by Flipping a Coin"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Computing Pi by Flipping a Coin}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
library(coinpi)
```

## Background

In the post
<https://statmodeling.stat.columbia.edu/2026/02/21/computing-pi-by-flipping-a-coin/>,
Andrew Gelman discusses a 2026 note by Jim Propp. The simulation rule is:

1. Flip a fair coin.
2. Keep flipping until heads first outnumber tails.
3. Record `H_tau / tau` for that trial (`H_tau` heads out of `tau` tosses).
4. Repeat and average.

The theorem in the note states that:

`E(H_tau / tau) = pi / 4`

so `4 * mean(H_tau / tau)` estimates `pi`.

## Why Chunked Vectorization?

A direct `while` loop over one trial at a time is simple but slow in R. This
package instead:

1. Simulates many active trials at once in a vectorized chunk.
2. Updates heads/tosses/balance vectors for all active trials each step.
3. Drops completed trials and continues until the chunk is done.
4. Carries running sums to the next chunk so total memory stays bounded.

With `chunk_size = 1e6L`, the algorithm processes one million trial states at a
time, then folds those results into the global estimate before moving on.

## Basic Usage

```{r}
fit <- simulate_coin_pi(
  n_sims = 2000L,
  chunk_size = 500L,
  seed = 123,
  progress = FALSE
)
fit
```

## Large Chunk Example

```{r, eval = FALSE}
# One million trial states per chunk
fit_big <- simulate_coin_pi(
  n_sims = 5e6L,
  chunk_size = 1e6L,
  seed = 2026,
  progress = TRUE
)
fit_big$pi_hat
```

## Returning Raw Ratios

```{r}
fit_ratios <- simulate_coin_pi(
  n_sims = 2000L,
  chunk_size = 500L,
  seed = 99,
  progress = FALSE,
  return_ratios = TRUE
)

summary(fit_ratios$ratios)
```
